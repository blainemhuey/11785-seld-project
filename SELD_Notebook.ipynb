{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchaudio\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "0od7MgU6ngPU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T4CGAo7g5zH",
        "outputId": "14568f78-9e94-4747-932f-2e6ef2befbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://zenodo.org/record/6387880/files/foa_dev.zip?download=1 --output /content/gdrive/MyDrive/ProjectData/foa_dev.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPEnORZ6hAxm",
        "outputId": "d2670467-dec5-438b-948d-f4fc22d4f3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2097M  100 2097M    0     0  20.8M      0  0:01:40  0:01:40 --:--:-- 27.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://zenodo.org/record/6387880/files/metadata_dev.zip?download=1 --output /content/gdrive/MyDrive/ProjectData/metadata_dev.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdCVsOzSjcL_",
        "outputId": "e29f1daf-b4b4-46c4-8664-9c06975eac64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  619k  100  619k    0     0   334k      0  0:00:01  0:00:01 --:--:--  334k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/gdrive/MyDrive/ProjectData/foa_dev.zip -d /content/gdrive/MyDrive/ProjectData/data"
      ],
      "metadata": {
        "id": "SGtOHzsplak8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/gdrive/MyDrive/ProjectData/metadata_dev.zip -d /content/gdrive/MyDrive/ProjectData/data\n"
      ],
      "metadata": {
        "id": "kv2_F49SlvpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "SOUND_EVENT_CLASSES = [\n",
        "    \"Female speech, woman speaking\",\n",
        "    \"Male speech, man speaking\",\n",
        "    \"Clapping\",\n",
        "    \"Telephone\",\n",
        "    \"Laughter\",\n",
        "    \"Domestic sounds\",\n",
        "    \"Walk, footsteps\",\n",
        "    \"Door, open or close\",\n",
        "    \"Music\",\n",
        "    \"Musical instrument\",\n",
        "    \"Water tap, faucet\",\n",
        "    \"Bell\",\n",
        "    \"Knock\"\n",
        "]"
      ],
      "metadata": {
        "id": "o64yGQJ36pra"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \n",
        "    \"track\" : 3,\n",
        "    \"classes\" : len(SOUND_EVENT_CLASSES),\n",
        "    \n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "aB4UQ0nm6iRm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89AXijawcFsW",
        "outputId": "c2b4c62c-4809-46e7-cfd3-4fb807f438d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classes': 13, 'track': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "SOUND_EVENT_CLASSES = [\n",
        "    \"Female speech, woman speaking\",\n",
        "    \"Male speech, man speaking\",\n",
        "    \"Clapping\",\n",
        "    \"Telephone\",\n",
        "    \"Laughter\",\n",
        "    \"Domestic sounds\",\n",
        "    \"Walk, footsteps\",\n",
        "    \"Door, open or close\",\n",
        "    \"Music\",\n",
        "    \"Musical instrument\",\n",
        "    \"Water tap, faucet\",\n",
        "    \"Bell\",\n",
        "    \"Knock\"\n",
        "]\n",
        "\n",
        "\n",
        "class FOADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for DCASE FOA Datsets\n",
        "    \"\"\"\n",
        "\n",
        "    implemented_model_features = [\"seldnet\", \"rd3net\"]\n",
        "\n",
        "    def __init__(self, data_path, folds=None, train=True, model=\"seldnet\", hop_length=20, context=0):\n",
        "        \"\"\"\n",
        "        Init Function for FOADataset\n",
        "        :param data_path: String path to root folder containing 'foa_dev' and 'metadata_dev'\n",
        "        :param folds: List of fold integers to use in this dataset\n",
        "        :param train: Bool indicating whether to use train dataset of val dataset\n",
        "        \"\"\"\n",
        "\n",
        "        # Assert that requested features are currently implemented\n",
        "        assert model in FOADataset.implemented_model_features\n",
        "        feat_size = 250\n",
        "        hop_length = 20\n",
        "        self.model = model\n",
        "        # Calculate Directory Names\n",
        "        foa_directory_sony = osp.join(data_path, \"foa_dev\", \"dev-train-sony\" if train else \"dev-test-sony\")\n",
        "        meta_directory_sony = osp.join(data_path, \"metadata_dev\", \"dev-train-sony\" if train else \"dev-test-sony\")\n",
        "        foa_directory_tau = osp.join(data_path, \"foa_dev\", \"dev-train-tau\" if train else \"dev-test-tau\")\n",
        "        meta_directory_tau = osp.join(data_path, \"metadata_dev\", \"dev-train-tau\" if train else \"dev-test-tau\")\n",
        "\n",
        "        all_foa_files = [osp.join(foa_directory_tau, file) for file in os.listdir(foa_directory_tau)]\n",
        "        all_foa_files.extend([osp.join(foa_directory_sony, file) for file in os.listdir(foa_directory_sony)])\n",
        "        all_meta_files = [osp.join(meta_directory_tau, file) for file in os.listdir(meta_directory_tau)]\n",
        "        all_meta_files.extend([osp.join(meta_directory_sony, file) for file in os.listdir(meta_directory_sony)])\n",
        "\n",
        "        # Parse File Names\n",
        "        foa_file_data = [self.parse_foa_file_name(file) for file in all_foa_files]\n",
        "        meta_file_data = [self.parse_foa_file_name(file) for file in all_meta_files]\n",
        "\n",
        "        # Create Lists of All Valid File Paths in Given Folds\n",
        "        self.folds = folds\n",
        "        self.foa_files = [\n",
        "            file for file, data in zip(all_foa_files, foa_file_data)\n",
        "            if (folds is None or data[\"fold\"] in folds)\n",
        "        ]\n",
        "        self.foa_files.sort()\n",
        "        self.meta_files = [\n",
        "            file for file, data in zip(all_meta_files, meta_file_data)\n",
        "            if (folds is None or data[\"fold\"] in folds)\n",
        "        ]\n",
        "        self.meta_files.sort()\n",
        "        self.foa_files = self.foa_files[:10]\n",
        "        self.meta_files = self.meta_files[:10]\n",
        "\n",
        "        assert len(self.foa_files) == len(self.meta_files)\n",
        "\n",
        "        # Load SELDNet Input Features and ACCDOA Output\n",
        "        features = []\n",
        "        multi_accdoas = []\n",
        "        self.feature_width = 100 // hop_length\n",
        "        for foa_file, meta_file in zip(self.foa_files, self.meta_files):\n",
        "            if model == \"seldnet\":\n",
        "                feature = self.audio_to_seldnet_features(foa_file, hop_length=hop_length)[:,:, :-1]\n",
        "                multi_accdoa = self.metadata_to_multi_accdoa(self.load_metadata(meta_file),\n",
        "                                                         total_frames=feature.shape[2] // (100 // 20))[:,:, :-1]\n",
        "                feature_chunked = self.chunk_seldnet_feature(feature, feat_size)\n",
        "                multi_accdoa_chunked = self.chunk_seldnet_multiaccdoa(multi_accdoa, feat_size, hop_length )\n",
        "                # print(len(feature_chunked))\n",
        "                # print(len(multi_accdoa_chunked))\n",
        "                # print(feature.shape)\n",
        "                # print(multi_accdoa.shape)\n",
        "\n",
        "                assert(len(feature_chunked) == len(multi_accdoa_chunked))\n",
        "                features.extend(feature_chunked)\n",
        "                multi_accdoas.extend(multi_accdoa_chunked)\n",
        "\n",
        "            else:\n",
        "                feature = self.audio_to_rd3net_features(foa_file, hop_length=hop_length)\n",
        "                total_frames = feature.shape[2] // (100 // hop_length)\n",
        "                feature = feature[:, :, :total_frames * (100 // hop_length)]\n",
        "                multi_accdoa = self.metadata_to_multi_accdoa(self.load_metadata(meta_file),\n",
        "                                                             total_frames=total_frames)\n",
        "                features.append(feature)\n",
        "                multi_accdoas.append(multi_accdoa)\n",
        "\n",
        "        if model==\"seldnet\":\n",
        "            self.features = np.stack(features)\n",
        "            self.multi_accdoa = np.stack(multi_accdoas)\n",
        "        else:\n",
        "            self.features = pad(torch.concat(features, dim=-1), (context, context))\n",
        "            self.multi_accdoa = np.concatenate(multi_accdoas, axis=-1)\n",
        "        self.context = context\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_foa_file_name(file):\n",
        "        \"\"\"\n",
        "        Parses filenames of the following format:\n",
        "        \"fold[fold number]_room[room number per fold]_mix[recording number per room per split].wav\"\n",
        "        :param file: filename\n",
        "        :return: metadata dictionary\n",
        "        \"\"\"\n",
        "\n",
        "        name, extension = osp.splitext(osp.basename(file))\n",
        "        fold_text, room_text, mix_text = name.split(\"_\")\n",
        "        fold = int(fold_text.replace(\"fold\", \"\"))\n",
        "        room = int(room_text.replace(\"room\", \"\"))\n",
        "        mix = int(mix_text.replace(\"mix\", \"\"))\n",
        "        return {\"fold\": fold, \"room\": room, \"mix\": mix}\n",
        "\n",
        "    @staticmethod\n",
        "    def audio_to_seldnet_features(file, fft_size=1024, hop_length=20, eps=1e-8):\n",
        "        \"\"\"\n",
        "        Generates the SELDNet Input Features\n",
        "        :param file: Filepath to Audio File to Load\n",
        "        :param fft_size: Size of FFT calculation to perform\n",
        "        :param hop_length: Stride of FFT in ms\n",
        "        :param eps: Division eps to prevent NaN outputs\n",
        "        :return: torch.Tensor of Shape 7x64xT\n",
        "        \"\"\"\n",
        "        waveform, sample_rate = torchaudio.load(file, normalize=True)\n",
        "\n",
        "        spec_trans = torchaudio.transforms.Spectrogram(n_fft=fft_size, hop_length=sample_rate // (1000 // hop_length),\n",
        "                                                       pad=0, power=None)\n",
        "        mel_trans = torchaudio.transforms.MelScale(n_mels=64, sample_rate=sample_rate, n_stft=fft_size // 2 + 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            spectrogram = spec_trans(waveform)\n",
        "            mel_spec = mel_trans(torch.real(torch.pow(spectrogram, 2)))\n",
        "\n",
        "            intensity = torch.real(torch.conj(spectrogram[0]) * spectrogram[1:])\n",
        "            intensity = intensity / (torch.pow(torch.abs(spectrogram[0]), 2) +\n",
        "                                     torch.mean(torch.pow(torch.abs(spectrogram[1:]), 2), dim=0) + eps)\n",
        "            mel_intensity = mel_trans(intensity)\n",
        "        return torch.concat((mel_spec, mel_intensity), dim=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def audio_to_rd3net_features(file, fft_size=1024, hop_length=20):\n",
        "        \"\"\"\n",
        "        Generates the RD3Net Input Features\n",
        "        :param file: Filepath to Audio File to Load\n",
        "        :param fft_size: Size of FFT calculation to perform\n",
        "        :param hop_length: Stride of FFT in ms\n",
        "        :return: torch.Tensor of Shape 7x(fft/2+1)xT\n",
        "        \"\"\"\n",
        "        waveform, sample_rate = torchaudio.load(file, normalize=True)\n",
        "\n",
        "        spec_trans = torchaudio.transforms.Spectrogram(n_fft=fft_size, hop_length=sample_rate // (1000 // hop_length),\n",
        "                                                       pad=0, power=None)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            spectrogram = spec_trans(waveform)\n",
        "\n",
        "            amplitude = torch.abs(spectrogram)\n",
        "            ipd = torch.angle(spectrogram[0]) - torch.angle(spectrogram[1:])\n",
        "\n",
        "        return torch.concat((amplitude, ipd), dim=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_metadata(file):\n",
        "        \"\"\"\n",
        "        Reads in the CSV Label File of the Format\n",
        "        '[frame number (int)], [active class index (int)], [source number index (int)], [azimuth (int)], [elevation (int)]'\n",
        "\n",
        "        :param file: Filepath to CSV File to Load\n",
        "        :return: List of Metadata Dictionaries\n",
        "        \"\"\"\n",
        "        metadata = []\n",
        "        with open(file, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                frame_number, active_class, source_number, azimuth, elevation = line.split(\",\")\n",
        "                metadata.append({\n",
        "                    \"frame_number\": int(frame_number),\n",
        "                    \"active_class\": int(active_class),\n",
        "                    \"source_number\": int(source_number),\n",
        "                    \"azimuth\": int(azimuth),\n",
        "                    \"elevation\": int(elevation)\n",
        "                })\n",
        "        return metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def metadata_to_multi_accdoa(metadata, total_frames, n=3, c=len(SOUND_EVENT_CLASSES)):\n",
        "        \"\"\"\n",
        "        Turns a List of Python Dictionaries with SELD Labels Into A Multi-ACCDOA Truth Vector\n",
        "        :param metadata: List of Python Dictionaries (from 'load_metadata')\n",
        "        :param total_frames: Total number of 100ms frames in source audio\n",
        "        :param n: Maximum number of repetitions\n",
        "        :param c: Number of classes\n",
        "        :return: N x 3 x C x Total Frames Numpy Ndarray\n",
        "        \"\"\"\n",
        "        multi_accdoa = np.zeros((n, 3, c, total_frames))\n",
        "        event_count_per_frame = np.zeros((c, total_frames), dtype=np.int)\n",
        "        for metadata_i in metadata:\n",
        "            f, a, s, az, el = (metadata_i[\"frame_number\"], metadata_i[\"active_class\"], metadata_i[\"source_number\"],\n",
        "                               metadata_i[\"azimuth\"], metadata_i[\"elevation\"])\n",
        "            f -= 1\n",
        "            norm_az_el = np.array([np.cos(np.deg2rad(az)), np.sin(np.deg2rad(az)), np.sin(np.deg2rad(el))])\n",
        "            multi_accdoa[event_count_per_frame[a, f]:, :, a, f] = norm_az_el\n",
        "            event_count_per_frame[a, f] += 1\n",
        "        return multi_accdoa\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_seldnet_feature(feature, feat_size=250):\n",
        "      \n",
        "      s0,s1,s2 = feature.shape\n",
        "      # print(feature.shape)\n",
        "      news2 = int(np.ceil(s2/feat_size)*feat_size)\n",
        "      # print(\"padded length  \", news2)\n",
        "      feature = np.pad(feature, ((0,0), (0,0), (0,news2-s2)))\n",
        "      # print(feature.shape, \"  new feature shape\")\n",
        "      feature = np.reshape(feature, (7,news2,64))\n",
        "      return np.split(feature, news2/feat_size, axis=1 )\n",
        "      # return feature\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_seldnet_multiaccdoa(multi_accdoa,feat_size, hop_length):\n",
        "      split_size = feat_size//(100//hop_length)\n",
        "      # print(multi_accdoa.shape, \"  multi accdoa shape\")\n",
        "      # print(split_size, \" split size\")\n",
        "      split_count = multi_accdoa.shape[-1]/split_size\n",
        "      toPad = int(np.ceil(split_count)*split_size) - multi_accdoa.shape[-1]\n",
        "\n",
        "      multi_accdoa = np.pad(multi_accdoa, ((0,0), (0,0),(0,0), (0,toPad)))\n",
        "      # print(multi_accdoa.shape, \"  multi accdoa shape\")\n",
        "      split_count = multi_accdoa.shape[-1]/split_size\n",
        "      # print(split_count)\n",
        "\n",
        "\n",
        "      return np.split(multi_accdoa, split_count, axis=-1)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.model==\"seldnet\":\n",
        "            return self.features.shape[0]\n",
        "        return self.multi_accdoa.shape[-1]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if self.model ==\"seldnet\":\n",
        "            return torch.from_numpy(self.features[item]), torch.from_numpy(self.multi_accdoa[item])\n",
        "        return self.features[:, :, item*self.feature_width:(item+1)*self.feature_width+self.context*2], \\\n",
        "               self.multi_accdoa[:, :, :, item]\n"
      ],
      "metadata": {
        "id": "-704lZgYmJQr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oH7XUvTtYvXk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = FOADataset(\"/content/gdrive/MyDrive/ProjectData/data\")\n",
        "train_loader = torch.utils.data.DataLoader( train_data , batch_size= 128, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "print(len(train_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Di4BasfnK0x",
        "outputId": "f047a2b2-4006-4fec-efa7-589c03426a15"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:213: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dap44TGuqosJ",
        "outputId": "a000cd88-ee26-4aa1-fabc-420b1cf0ebda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(190, 7, 250, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To import files, on colab I am keeping them in the ProjectData folder might not be needed locally"
      ],
      "metadata": {
        "id": "q_PIrhyEvnZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys  \n",
        "# sys.path.insert(0, '/content/gdrive/MyDrive/ProjectData')"
      ],
      "metadata": {
        "id": "7eRzCuQynunX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from foa_dataset import FOADataset as FOADataset2"
      ],
      "metadata": {
        "id": "8qiZOA1OvAt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data2 = FOADataset2(\"/content/gdrive/MyDrive/ProjectData/data\")\n",
        "# train_loader = torch.utils.data.DataLoader( train_data , batch_size= 2, shuffle=True)\n",
        "\n",
        "\n",
        "# print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "XGX-38qXvzEK",
        "outputId": "a5806c7b-ff1b-4ed2-d3c9-edab8f4e444d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9abee3bd4de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFOADataset2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/ProjectData/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ProjectData/foa_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, folds, train)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_to_foa_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoa_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             multi_accdoa = self.metadata_to_multi_accdoa(self.load_metadata(meta_file),\n\u001b[0;32m---> 77\u001b[0;31m                                                          total_frames=feature.shape[2])\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mmulti_accdoas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_accdoa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ProjectData/foa_dataset.py\u001b[0m in \u001b[0;36mmetadata_to_multi_accdoa\u001b[0;34m(metadata, total_frames, n, c)\u001b[0m\n\u001b[1;32m    151\u001b[0m                                metadata_i[\"azimuth\"], metadata_i[\"elevation\"])\n\u001b[1;32m    152\u001b[0m             \u001b[0mnorm_az_el\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mmulti_accdoa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_count_per_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_az_el\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mevent_count_per_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmulti_accdoa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (3,1) into shape (2,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tor\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "\tdef __init__(self, max_pool = (5,4), out_filter=64, in_filter=64, kernel_size=3, dropout_rate=0.01):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.max_pool = max_pool\n",
        "\t\tself.out_filter = out_filter\n",
        "\t\tself.conv = nn.Conv2d(in_filter,out_filter, kernel_size=kernel_size, padding=(1,1))\n",
        "\t\tself.bn = nn.BatchNorm2d(out_filter)\n",
        "\t\tself.mpool = nn.MaxPool2d(self.max_pool)\n",
        "\t\tself.dropout = nn.Dropout2d(dropout_rate)\n",
        "\n",
        "\tdef forward(self,x):\n",
        "\t\tx = self.conv(x)\n",
        "\t\tx = self.bn(x)\n",
        "\t\tx = nn.ReLU()(x)\n",
        "\t\tx = self.mpool(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "\n",
        "class Network_Seldnet(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# print(\"Here\")\n",
        "\t\tmax_pool_list = [(5,4),(1,4),(1,2)]\n",
        "\t\tself.conv_list = nn.ModuleList()\n",
        "\t\tfor i,pool in enumerate(max_pool_list):\n",
        "\t\t\t# print(\"adding pool \")\n",
        "\t\t\tif i == 0:\n",
        "\t\t\t\tself.conv_list.append(\n",
        "\t\t\t\t\tConvBlock(pool, 64,7)\n",
        "\t\t\t\t)\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.conv_list.append(\n",
        "\t\t\t\t\tConvBlock(pool)\n",
        "\t\t\t\t)\n",
        "\t\tprint(len(self.conv_list))\n",
        "\n",
        "\t\tconv_out = 64*int(64/(4*4*2))\n",
        "\t\tself.rnn = nn.GRU(conv_out, 128, num_layers=2, bidirectional=True, batch_first=True, dropout=0.01)\n",
        "\t\tself.rnn_act = nn.Tanh()\n",
        "\n",
        "\t\tself.linear = nn.Linear(128,3*config[\"track\"] *config[\"classes\"])\n",
        "\t\tself.linear1 = nn.Linear(128,128)\n",
        "\t\tself.act1 = nn.Tanh()\n",
        "\t\tself.act = nn.Tanh()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tprint(\"Forward\")\n",
        "\t\tprint(len(self.conv_list))\n",
        "\t\tfor i in range(len(self.conv_list)):\n",
        "\t\t\tx = self.conv_list[i](x)\n",
        "\t\tprint(\" Post conv list\")\n",
        "\t\n",
        "\t\tx = x.transpose(1, 2).contiguous()\n",
        "\t\tx = x.view(x.shape[0], x.shape[1], -1).contiguous()\n",
        "\t\tx,_ = self.rnn(x)\n",
        "\t\tx = self.rnn_act(x)\n",
        "\t\n",
        "\t\tx = x[:, :, x.shape[-1]//2:] * x[:, :, :x.shape[-1]//2]\n",
        "\n",
        "\t\tx = self.linear1(x)\n",
        "\t\tx = self.act1(x)\n",
        "\t\n",
        "\t\tx = self.linear(x)\n",
        "\t\tx = self.act(x)\n",
        "\t\t\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2pXoNbGawI-3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data.features))\n",
        "print(len(train_data.multi_accdoa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AePK8SJWXmc9",
        "outputId": "c627dd89-d729-4caf-adaf-0dcc1510e2f6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189\n",
            "189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional\n",
        "# Test code for checking shapes and return arguments of the train and val loaders\n",
        "for data in train_loader:\n",
        "    x, y = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYAkTo8K7C3b",
        "outputId": "791350a7-7df2-4f09-f21c-47d9d4400899"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 7, 250, 64]) torch.Size([128, 3, 3, 12, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4TK4TYP8kjY",
        "outputId": "0c2eaf63-a7a8-40df-8d9c-ccce277f7e20"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummaryX # We also install a summary package to check our model's forward before training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snElDB1s8-qj",
        "outputId": "3a099bf8-d131-4f69-8b79-a3ce6edacb89"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (3.10.0.2)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummaryX import summary\n"
      ],
      "metadata": {
        "id": "nboll0M99wme"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network_Seldnet().to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5z7lvzg48Skl",
        "outputId": "53ffc513-3ff4-42b7-bfc6-46c254b307bd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Network_Seldnet(\n",
            "  (conv_list): ModuleList(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (mpool): MaxPool2d(kernel_size=(5, 4), stride=(5, 4), padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout2d(p=0.01, inplace=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (mpool): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout2d(p=0.01, inplace=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (mpool): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout2d(p=0.01, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (rnn): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.01, bidirectional=True)\n",
            "  (rnn_act): Tanh()\n",
            "  (linear): Linear(in_features=128, out_features=117, bias=True)\n",
            "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (act1): Tanh()\n",
            "  (act): Tanh()\n",
            ")\n",
            "Forward\n",
            "3\n",
            " Post conv list\n",
            "==========================================================================================\n",
            "                                    Kernel Shape        Output Shape  \\\n",
            "Layer                                                                  \n",
            "0_conv_list.0.Conv2d_conv          [7, 64, 3, 3]  [128, 64, 250, 64]   \n",
            "1_conv_list.0.BatchNorm2d_bn                [64]  [128, 64, 250, 64]   \n",
            "2_conv_list.0.MaxPool2d_mpool                  -   [128, 64, 50, 16]   \n",
            "3_conv_list.0.Dropout2d_dropout                -   [128, 64, 50, 16]   \n",
            "4_conv_list.1.Conv2d_conv         [64, 64, 3, 3]   [128, 64, 50, 16]   \n",
            "5_conv_list.1.BatchNorm2d_bn                [64]   [128, 64, 50, 16]   \n",
            "6_conv_list.1.MaxPool2d_mpool                  -    [128, 64, 50, 4]   \n",
            "7_conv_list.1.Dropout2d_dropout                -    [128, 64, 50, 4]   \n",
            "8_conv_list.2.Conv2d_conv         [64, 64, 3, 3]    [128, 64, 50, 4]   \n",
            "9_conv_list.2.BatchNorm2d_bn                [64]    [128, 64, 50, 4]   \n",
            "10_conv_list.2.MaxPool2d_mpool                 -    [128, 64, 50, 2]   \n",
            "11_conv_list.2.Dropout2d_dropout               -    [128, 64, 50, 2]   \n",
            "12_rnn                                         -      [128, 50, 256]   \n",
            "13_rnn_act                                     -      [128, 50, 256]   \n",
            "14_linear1                            [128, 128]      [128, 50, 128]   \n",
            "15_act1                                        -      [128, 50, 128]   \n",
            "16_linear                             [128, 117]      [128, 50, 117]   \n",
            "17_act                                         -      [128, 50, 117]   \n",
            "\n",
            "                                    Params Mult-Adds  \n",
            "Layer                                                 \n",
            "0_conv_list.0.Conv2d_conv           4.096k   64.512M  \n",
            "1_conv_list.0.BatchNorm2d_bn         128.0      64.0  \n",
            "2_conv_list.0.MaxPool2d_mpool            -         -  \n",
            "3_conv_list.0.Dropout2d_dropout          -         -  \n",
            "4_conv_list.1.Conv2d_conv          36.928k  29.4912M  \n",
            "5_conv_list.1.BatchNorm2d_bn         128.0      64.0  \n",
            "6_conv_list.1.MaxPool2d_mpool            -         -  \n",
            "7_conv_list.1.Dropout2d_dropout          -         -  \n",
            "8_conv_list.2.Conv2d_conv          36.928k   7.3728M  \n",
            "9_conv_list.2.BatchNorm2d_bn         128.0      64.0  \n",
            "10_conv_list.2.MaxPool2d_mpool           -         -  \n",
            "11_conv_list.2.Dropout2d_dropout         -         -  \n",
            "12_rnn                            494.592k   491.52k  \n",
            "13_rnn_act                               -         -  \n",
            "14_linear1                         16.512k   16.384k  \n",
            "15_act1                                  -         -  \n",
            "16_linear                          15.093k   14.976k  \n",
            "17_act                                   -         -  \n",
            "------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params             604.533k\n",
            "Trainable params         604.533k\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             101.899072M\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    Kernel Shape        Output Shape  \\\n",
              "Layer                                                                  \n",
              "0_conv_list.0.Conv2d_conv          [7, 64, 3, 3]  [128, 64, 250, 64]   \n",
              "1_conv_list.0.BatchNorm2d_bn                [64]  [128, 64, 250, 64]   \n",
              "2_conv_list.0.MaxPool2d_mpool                  -   [128, 64, 50, 16]   \n",
              "3_conv_list.0.Dropout2d_dropout                -   [128, 64, 50, 16]   \n",
              "4_conv_list.1.Conv2d_conv         [64, 64, 3, 3]   [128, 64, 50, 16]   \n",
              "5_conv_list.1.BatchNorm2d_bn                [64]   [128, 64, 50, 16]   \n",
              "6_conv_list.1.MaxPool2d_mpool                  -    [128, 64, 50, 4]   \n",
              "7_conv_list.1.Dropout2d_dropout                -    [128, 64, 50, 4]   \n",
              "8_conv_list.2.Conv2d_conv         [64, 64, 3, 3]    [128, 64, 50, 4]   \n",
              "9_conv_list.2.BatchNorm2d_bn                [64]    [128, 64, 50, 4]   \n",
              "10_conv_list.2.MaxPool2d_mpool                 -    [128, 64, 50, 2]   \n",
              "11_conv_list.2.Dropout2d_dropout               -    [128, 64, 50, 2]   \n",
              "12_rnn                                         -      [128, 50, 256]   \n",
              "13_rnn_act                                     -      [128, 50, 256]   \n",
              "14_linear1                            [128, 128]      [128, 50, 128]   \n",
              "15_act1                                        -      [128, 50, 128]   \n",
              "16_linear                             [128, 117]      [128, 50, 117]   \n",
              "17_act                                         -      [128, 50, 117]   \n",
              "\n",
              "                                    Params   Mult-Adds  \n",
              "Layer                                                   \n",
              "0_conv_list.0.Conv2d_conv           4096.0  64512000.0  \n",
              "1_conv_list.0.BatchNorm2d_bn         128.0        64.0  \n",
              "2_conv_list.0.MaxPool2d_mpool          NaN         NaN  \n",
              "3_conv_list.0.Dropout2d_dropout        NaN         NaN  \n",
              "4_conv_list.1.Conv2d_conv          36928.0  29491200.0  \n",
              "5_conv_list.1.BatchNorm2d_bn         128.0        64.0  \n",
              "6_conv_list.1.MaxPool2d_mpool          NaN         NaN  \n",
              "7_conv_list.1.Dropout2d_dropout        NaN         NaN  \n",
              "8_conv_list.2.Conv2d_conv          36928.0   7372800.0  \n",
              "9_conv_list.2.BatchNorm2d_bn         128.0        64.0  \n",
              "10_conv_list.2.MaxPool2d_mpool         NaN         NaN  \n",
              "11_conv_list.2.Dropout2d_dropout       NaN         NaN  \n",
              "12_rnn                            494592.0    491520.0  \n",
              "13_rnn_act                             NaN         NaN  \n",
              "14_linear1                         16512.0     16384.0  \n",
              "15_act1                                NaN         NaN  \n",
              "16_linear                          15093.0     14976.0  \n",
              "17_act                                 NaN         NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-019d86f6-5681-4845-b4ff-363ac7c46083\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_conv_list.0.Conv2d_conv</th>\n",
              "      <td>[7, 64, 3, 3]</td>\n",
              "      <td>[128, 64, 250, 64]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>64512000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_conv_list.0.BatchNorm2d_bn</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[128, 64, 250, 64]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_conv_list.0.MaxPool2d_mpool</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 64, 50, 16]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_conv_list.0.Dropout2d_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 64, 50, 16]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_conv_list.1.Conv2d_conv</th>\n",
              "      <td>[64, 64, 3, 3]</td>\n",
              "      <td>[128, 64, 50, 16]</td>\n",
              "      <td>36928.0</td>\n",
              "      <td>29491200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_conv_list.1.BatchNorm2d_bn</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[128, 64, 50, 16]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_conv_list.1.MaxPool2d_mpool</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 64, 50, 4]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_conv_list.1.Dropout2d_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 64, 50, 4]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_conv_list.2.Conv2d_conv</th>\n",
              "      <td>[64, 64, 3, 3]</td>\n",
              "      <td>[128, 64, 50, 4]</td>\n",
              "      <td>36928.0</td>\n",
              "      <td>7372800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_conv_list.2.BatchNorm2d_bn</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[128, 64, 50, 4]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_conv_list.2.MaxPool2d_mpool</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 64, 50, 2]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_conv_list.2.Dropout2d_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 64, 50, 2]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_rnn</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 50, 256]</td>\n",
              "      <td>494592.0</td>\n",
              "      <td>491520.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_rnn_act</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 50, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_linear1</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>[128, 50, 128]</td>\n",
              "      <td>16512.0</td>\n",
              "      <td>16384.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_act1</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 50, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_linear</th>\n",
              "      <td>[128, 117]</td>\n",
              "      <td>[128, 50, 117]</td>\n",
              "      <td>15093.0</td>\n",
              "      <td>14976.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_act</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 50, 117]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-019d86f6-5681-4845-b4ff-363ac7c46083')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-019d86f6-5681-4845-b4ff-363ac7c46083 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-019d86f6-5681-4845-b4ff-363ac7c46083');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sharathadavanne/seld-dcase2022.git"
      ],
      "metadata": {
        "id": "9546YJC49mKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6eb1f5-8122-4039-e98a-3cacca2ef723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'seld-dcase2022'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 103 (delta 66), reused 65 (delta 31), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 1.06 MiB | 7.88 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys  \n",
        "sys.path.insert(0, '/content/seld-dcase2022')"
      ],
      "metadata": {
        "id": "7VbLkjzTbpJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/seld-dcase2022/batch_feature_extraction.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MytxeoQQbBuH",
        "outputId": "a27c1239-0bf4-4887-caf9-dc236d667bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16: fold4_room24_mix003.wav, (4865, 448)\n",
            "17: fold4_room24_mix005.wav, (6865, 448)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/seld-dcase2022/batch_feature_extraction.py\", line 30, in <module>\n",
            "    sys.exit(main(sys.argv))\n",
            "  File \"/content/seld-dcase2022/batch_feature_extraction.py\", line 22, in main\n",
            "    dev_feat_cls.extract_all_feature()\n",
            "  File \"/content/seld-dcase2022/cls_feature_class.py\", line 380, in extract_all_feature\n",
            "    self.extract_file_feature((file_cnt, wav_path, feat_path))\n",
            "  File \"/content/seld-dcase2022/cls_feature_class.py\", line 338, in extract_file_feature\n",
            "    mel_spect = self._get_mel_spectrogram(spect)\n",
            "  File \"/content/seld-dcase2022/cls_feature_class.py\", line 135, in _get_mel_spectrogram\n",
            "    log_mel_spectra = librosa.power_to_db(mel_spectra)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py\", line 1559, in power_to_db\n",
            "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat = np.load('/content/gdrive/MyDrive/ProjectData/data/seld_feat_label/foa_dev/fold4_room23_mix002.npy')"
      ],
      "metadata": {
        "id": "ouSmiK1jbjc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12LWsIX4cV0Z",
        "outputId": "5665bef6-141e-4a96-c215-fcd76b4eba3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3035, 448)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQwhELkOcWre",
        "outputId": "73c6ade8-8be4-40e5-893a-670851e21352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2235, 448)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/seld-dcase2022/train_seldnet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUJqQy7ica3V",
        "outputId": "3a0dbca6-58b1-4928-b002-134ed71f6826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/seld-dcase2022/train_seldnet.py']\n",
            "\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "The code expected two optional inputs\n",
            "\t>> python seld.py <task-id> <job-id>\n",
            "\t\t<task-id> is used to choose the user-defined parameter set from parameter.py\n",
            "Using default inputs for now\n",
            "\t\t<job-id> is a unique identifier which is used for output filenames (models, training plots). You can use any number or string for this.\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "SET: 1\n",
            "USING DEFAULT PARAMETERS\n",
            "\n",
            "\tquick_test: True\n",
            "\tfinetune_mode: False\n",
            "\tpretrained_model_weights: models/1_1_foa_dev_split6_model.h5\n",
            "\tdataset_dir: /content/gdrive/MyDrive/ProjectData/data\n",
            "\tunique_classes: 13\n",
            "\tfeat_label_dir: /content/gdrive/MyDrive/ProjectData/data/seld_feat_label\n",
            "\tmodel_dir: models/\n",
            "\tdcase_output_dir: results/\n",
            "\tmode: dev\n",
            "\tdataset: foa\n",
            "\tfs: 24000\n",
            "\thop_len_s: 0.02\n",
            "\tlabel_hop_len_s: 0.1\n",
            "\tmax_audio_len_s: 60\n",
            "\tnb_mel_bins: 64\n",
            "\tuse_salsalite: False\n",
            "\tfmin_doa_salsalite: 50\n",
            "\tfmax_doa_salsalite: 2000\n",
            "\tfmax_spectra_salsalite: 9000\n",
            "\tmulti_accdoa: True\n",
            "\tthresh_unify: 15\n",
            "\tlabel_sequence_length: 50\n",
            "\tbatch_size: 128\n",
            "\tdropout_rate: 0.05\n",
            "\tnb_cnn2d_filt: 64\n",
            "\tf_pool_size: [4, 4, 2]\n",
            "\tnb_rnn_layers: 2\n",
            "\trnn_size: 128\n",
            "\tself_attn: False\n",
            "\tnb_heads: 4\n",
            "\tnb_fnn_layers: 1\n",
            "\tfnn_size: 128\n",
            "\tnb_epochs: 100\n",
            "\tlr: 0.001\n",
            "\taverage: macro\n",
            "\tlad_doa_thresh: 20\n",
            "\tfeature_sequence_length: 250\n",
            "\tt_pool_size: [5, 1, 1]\n",
            "\tpatience: 100\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "------------------------------------      SPLIT [4]   -----------------------------------------------\n",
            "---------------------------------------------------------------------------------------------------\n",
            "unique_name: 1_1_dev_split0_multiaccdoa_foa\n",
            "\n",
            "---------------- SELD-net -------------------\n",
            "FEATURES:\n",
            "\tdata_in: (128, 7, 250, 64)\n",
            "\tdata_out: (128, 250, 117)\n",
            "\n",
            "MODEL:\n",
            "\tdropout_rate: 0.05\n",
            "\tCNN: nb_cnn_filt: 64, f_pool_size[4, 4, 2], t_pool_size[5, 1, 1]\n",
            "\trnn_size: 128, fnn_size: 128\n",
            "\n",
            "CRNN(\n",
            "  (conv_block_list): ModuleList(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): MaxPool2d(kernel_size=(5, 4), stride=(5, 4), padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Dropout2d(p=0.05, inplace=False)\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Dropout2d(p=0.05, inplace=False)\n",
            "    (6): ConvBlock(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Dropout2d(p=0.05, inplace=False)\n",
            "  )\n",
            "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.05, bidirectional=True)\n",
            "  (fnn_list): ModuleList(\n",
            "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=117, bias=True)\n",
            "  )\n",
            ")\n",
            "Dumping recording-wise val results in: results/1_1_dev_split0_multiaccdoa_foa_20220402103551_val\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/seld-dcase2022/train_seldnet.py\", line 387, in <module>\n",
            "    sys.exit(main(sys.argv))\n",
            "  File \"/content/seld-dcase2022/train_seldnet.py\", line 296, in main\n",
            "    score_obj = ComputeSELDResults(params)\n",
            "  File \"/content/seld-dcase2022/cls_compute_seld_results.py\", line 24, in __init__\n",
            "    gt_dict = self._feat_cls.load_output_format_file(os.path.join(self._desc_dir, split, ref_file))\n",
            "  File \"/content/seld-dcase2022/cls_feature_class.py\", line 468, in load_output_format_file\n",
            "    _words = _line.strip().split(',')\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EAD43Z3jhNsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WZM2vIH6hqJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}